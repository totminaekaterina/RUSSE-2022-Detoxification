{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_datasets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMq0pHB7faI2/OYm9Hqt3g1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/totminaekaterina/RUSSE-2022-Detoxification/blob/main/prepare_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers==4.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu0SPjTzhIwX",
        "outputId": "4a4abec1-fc73-43c7-b9bf-6d57223a4acf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.6.0\n",
            "  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (4.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0) (3.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk5jq5jkhUbq",
        "outputId": "24903fac-3b6d-4d18-8cfe-34f10c0b7e35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1ZP0MEmZfVK4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 200\n",
        "\n",
        "\n",
        "def get_word_tokens(text):\n",
        "    tokens = re.sub(r\"[^\\w\\s]\", \"\", text).split()\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "def get_similarities(model, tokenizer, input_texts, output_texts):\n",
        "    # Tokenize sentences\n",
        "    encoded_input = tokenizer(input_texts, padding=True, truncation=True,\n",
        "                              max_length=MAX_LENGTH, return_tensors=\"pt\").to(model.device)\n",
        "    encoded_output = tokenizer(output_texts, padding=True, truncation=True,\n",
        "                              max_length=MAX_LENGTH, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        model_emb_input = model(**encoded_input)\n",
        "        model_emb_output = model(**encoded_output)\n",
        "\n",
        "    # Perform pooling. In this case, mean pooling\n",
        "    input_embeddings = mean_pooling(model_emb_input, encoded_input[\"attention_mask\"]).cpu()\n",
        "    output_embeddings = mean_pooling(model_emb_output, encoded_output[\"attention_mask\"]).cpu()\n",
        "    similarity = cosine_similarity(input_embeddings, output_embeddings)\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def get_rougel(input_text, output_text):\n",
        "    \"\"\"\n",
        "    Returns rouge-l f-score\n",
        "    \"\"\"\n",
        "    rouge = Rouge()\n",
        "    scores = []\n",
        "    # try/except because of empty output or just dot (in dev_sents)\n",
        "    try:\n",
        "        score = rouge.get_scores(input_text, output_text)[0]\n",
        "        score = score[\"rouge-l\"][\"f\"]\n",
        "    except ValueError:  \n",
        "        score = 0.0\n",
        "    return score\n",
        "\n",
        "\n",
        "def set_random_seed(seed_val):\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "g9YfUK58itMW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/train.tsv\n",
        "!wget https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/dev.tsv\n",
        "!wget https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/test.tsv"
      ],
      "metadata": {
        "id": "70aYKxEvgfxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = Path(\"/content\")\n",
        "TRAIN_DIR = DATA_DIR / \"train.tsv\"\n",
        "\n",
        "DEV_PATH = DATA_DIR / \"dev.tsv\"\n",
        "TEST_PATH = DATA_DIR / \"test.tsv\"\n",
        "\n",
        "OUTPUT_DIR = Path(DATA_DIR / \"prepared_data\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "YACDfbemfawk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model for embeddings"
      ],
      "metadata": {
        "id": "LLYfFYX4fkZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
        "model = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
      ],
      "metadata": {
        "id": "LrdodBTCfgVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validation dataset"
      ],
      "metadata": {
        "id": "ptm23XuLfsr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv(DEV_PATH, sep=\"\\t\")\n",
        "dev_df.drop([\"neutral_comment2\"], axis=1, inplace=True)\n",
        "dev_df.drop([\"neutral_comment3\"], axis=1, inplace=True)\n",
        "dev_df.columns = [\"toxic_comment\", \"neutral_comment1\"]\n",
        "\n",
        "dev_df[\"cosine_sim\"] = dev_df.apply(lambda x: get_similarities(model, tokenizer, x[\"toxic_comment\"], x[\"neutral_comment1\"]),\n",
        "                                    axis=1)\n",
        "dev_df[\"cosine_sim\"] = dev_df[\"cosine_sim\"].apply(lambda x: x[0][0])\n",
        "dev_df[\"rouge_l\"] = dev_df.apply(lambda x: get_rougel(x[\"toxic_comment\"], x[\"neutral_comment1\"]), axis=1)\n",
        "dev_df[\"input_len\"] = dev_df[\"toxic_comment\"].apply(lambda x: len(get_word_tokens(x)))\n",
        "dev_df[\"output_len\"] = dev_df[\"neutral_comment1\"].apply(lambda x: len(get_word_tokens(x)))\n",
        "dev_df.to_csv(OUTPUT_DIR / \"dev_df_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "NAm2iSRAfiKG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train dataset"
      ],
      "metadata": {
        "id": "oOy5UMEkfyPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(TRAIN_DIR, sep=\"\\t\")\n",
        "train_df.drop([\"index\"], axis=1, inplace=True)\n",
        "train_df.drop([\"neutral_comment2\"], axis=1, inplace=True)\n",
        "train_df.drop([\"neutral_comment3\"], axis=1, inplace=True)\n",
        "train_df.columns = [\"toxic_comment\", \"neutral_comment1\"]\n",
        "\n",
        "train_df[\"cosine_sim\"] = train_df.apply(lambda x: get_similarities(model, tokenizer, x[\"toxic_comment\"], x[\"neutral_comment1\"]),\n",
        "                                        axis=1)\n",
        "train_df[\"cosine_sim\"] = train_df[\"cosine_sim\"].apply(lambda x: x[0][0])\n",
        "\n",
        "train_df[\"rouge_l\"] = train_df.apply(lambda x: get_rougel(x[\"toxic_comment\"], x[\"neutral_comment1\"]), axis=1)\n",
        "\n",
        "train_df[\"input_len\"] = train_df[\"toxic_comment\"].apply(lambda x: len(get_word_tokens(x)))\n",
        "train_df[\"output_len\"] = train_df[\"neutral_comment1\"].apply(lambda x: len(get_word_tokens(x)))"
      ],
      "metadata": {
        "id": "cjIkkCOsfv52"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# select data"
      ],
      "metadata": {
        "id": "HHxgqKREf8FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_part = train_df[\n",
        "                       (train_df[\"cosine_sim\"] < 0.99)\n",
        "                       & (train_df[\"cosine_sim\"] > 0.6)\n",
        "                       & (train_df[\"rouge_l\"] < 0.8)\n",
        "                       & (train_df[\"rouge_l\"] > 0.1)\n",
        "                       & (train_df[\"output_len\"] <= train_df[\"input_len\"])]\n",
        "\n",
        "train_df_part[[\"toxic_comment\", \"neutral_comment1\"]].to_csv(OUTPUT_DIR / \"train_df_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "f-cz0h1Tf9B_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(OUTPUT_DIR / \"train_df_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "xwk_dfRF_MjJ"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}